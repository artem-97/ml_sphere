{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 5 ноября 2019, 06:00 <br\\>\n",
    "**Штраф за опоздание:** -2 балла после 06:00 5 ноября, -4 балла после 06:00 12 ноября, -6 баллов после 06:00 19 ноября  -8 баллов после 06:00 26 ноября.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0919, Задание 2] Фамилия Имя.<br\\>\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение 1.\n",
    "$$w_{new} = w_{old} - \\alpha \\nabla_{w} Q(w_{old}) $$\n",
    "$$w_{new} = w_{old} - 2 \\alpha (\\frac{w_{old}}{C} - \\frac{1}{l}\\sum_{i=1}^{l}(x_{i}w_{i} - y_{i})x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение 2.\n",
    "$$w_{new} = w_{old} - \\alpha(\\frac{1}{l\\ln{2}}\\sum_{i=1}^{l}(<w, x_i> - y_i)x_i + 2\\frac{w_{old}}{C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение 3.\n",
    "a) \n",
    "$$\\frac{\\partial^2}{\\partial w^2} Q(w) = 2X^TX$$\n",
    "Для положителноопределенности нужно доказать, что:\n",
    "$$z^TX^TXz > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.$$\n",
    "Но тут написана норма квадрата вектора, поэтому, очевидно,\n",
    "\n",
    "$$z^TX^TXz = ||Xz||^2 > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.$$\n",
    "\n",
    "b)$$\\frac{\\partial}{\\partial w} Q(w) = \\frac{\\partial}{\\partial w}[y^Ty - y^TXw - w^TX^Ty + w^TX^TXw + \\frac{w^Tw}{C}].$$\n",
    "$$\\frac{\\partial}{\\partial w} Q(w) = -2X^Ty + 2X^TXw  + 2 \\frac{w}{C}= 0$$\n",
    "$$X^TXw=X^Ty - \\frac{w}{C}$$\n",
    "$$w=(X^TX)^{-1}(X^Ty - \\frac{w}{C})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение 4.\n",
    "\n",
    "Хотим подобрать функцию потерь такую, чтобы ответ модели на объекте равный вероятности того, что объект из класса +1 доставлял ей минимум.\n",
    "$$argmin \\frac{1}{n}\\sum_{i=1}^{l}L(y_i, b) = p(y=+1|x)$$\n",
    "Этот минимум достигается на логистической функции потерь.\n",
    "Очевидно, что это верно и для матожидания\n",
    "$$\\frac{\\partial E}{\\partial a(x_i)} = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение 6.\n",
    "Нет, не нужно. $w_0$ - это просто фиктивный признак, нужный, чтобы подвинуть гиперплоскость $<w,x> = 0$, $<w,x> + w_0 = 0$. Чтобы каждый раз отдельно не выделять $w_0$ мы пишем $<w,x> = 0$, помня, что признак $w_0$ добавлен в $w$. Очевидно, что $w_0$ просто показывает просто показывает сдвиг гиперплоскости и никак не влияет на ее форму. Поэтому его не нужно учитывать в регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6] [3, 3, 5, 8, 2, 8, 2]\n",
      "(array([3, 5, 0]), array([5, 2, 8]))\n",
      "(array([2, 4, 6]), array([3, 8, 3]))\n",
      "(array([1]), array([2]))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y          - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_batch = X.copy()\n",
    "    y_batch = y.copy()\n",
    "    if shuffle:\n",
    "        random.shuffle(X_batch)\n",
    "        random.shuffle(y_batch)\n",
    "    assert(len(X_batch) == len(y_batch))\n",
    "   \n",
    "    for i in range(0, len(X_batch), batch_size):\n",
    "        X_batch_out = np.asarray(X_batch[i: i + batch_size])\n",
    "        y_batch_out = np.asanyarray(y_batch[i: i + batch_size])\n",
    "        yield (X_batch_out, y_batch_out)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "X = [i for i in range(7)]\n",
    "y = [random.randint(0,10) for _ in range(len(X))]\n",
    "print(X, y)\n",
    "\n",
    "my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=3)\n",
    "for item in my_batch_generator:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(w):\n",
    "    return np.sum(w**2)\n",
    "\n",
    "def L1(w):\n",
    "    return np.sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1 / (1 + np.exp(-x)) \n",
    "    return sigm_value_x\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        l = X_batch.shape[0]\n",
    "        if self.model_type == 'lin_reg':\n",
    "            a = np.dot(X_batch, self.weights)\n",
    "            loss = np.sum(a - y_batch)**2 / l + L2(self.weights) / self.C \n",
    "        elif self.model_type == 'log_reg':\n",
    "            a = sigmoid(np.dot(X_batch, self.weights))\n",
    "            loss = -1 / l * np.sum((y_batch * log(a) + (1 - y_batch) * log(1 - a))) \n",
    "            loss += L2(self.weights) / self.C\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        l = X_batch.shape[0]\n",
    "        if self.model_type == 'lin_reg':\n",
    "            #Q' = 2X^T(Xw-y)\n",
    "            a = np.dot(X_batch, self.weights)\n",
    "            loss_grad = np.dot(a - y_batch, X_batch) / l + self.weights / self.C\n",
    "        elif self.model_type == 'log_reg':\n",
    "            a = sigmoid(np.dot(X_batch, self.weights))\n",
    "            loss_grad = np.dot(a - y_batch, X_batch) / (l * np.log(2)) + 2 * self.weights / self.C\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights = self.weights - self.alpha * new_grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        # Нужно инициализровать случайно веса\n",
    "        #self.weights = None\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        np.random.seed(0)\n",
    "        self.weights = np.random.uniform(0, 10, size=X.shape[1]).astype(np.float64)\n",
    "        \n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                #batch_loss = self.calc_loss(X_batch, y_batch)  # До градиентного шага или после\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        X_new = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        if self.model_type == 'lin_reg':\n",
    "            y_hat = np.dot(X_new, self.weights)\n",
    "        elif self.model_type == 'log_reg':\n",
    "            y_hat = sigmoid(np.dot(X_new, self.weights))\n",
    "        # Желательно здесь использовать матричные операции между X и весами, например, numpy.dot \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf):\n",
    "    x = np.linspace(-5, 8)\n",
    "    y = -(clf[0] + clf[1] * x) / clf[2]\n",
    "    plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9053996390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "# plot_decision_boundary(your_model)\n",
    "lin_reg = MySGDClassifier(batch_generator, alpha=0.01, max_epoch=20, C=10, model_type='lin_reg')\n",
    "lin_reg.fit(X, y)\n",
    "plot_decision_boundary(lin_reg.weights)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a(reg_type):\n",
    "    N = 100\n",
    "    alpha = [0.1, 0.01, 0.001, 0.0001 ]\n",
    "    for a in alpha:\n",
    "        reg = MySGDClassifier(alpha=a, batch_generator=batch_generator,max_epoch=1, C=20, model_type=reg_type)\n",
    "        reg.fit(X, y)\n",
    "        loss = []\n",
    "        steps = len(reg.errors_log['loss']) // N\n",
    "        for i in range(steps):\n",
    "            loss.append(np.mean(reg.errors_log['loss'][N * i :N *(i + 1)]))\n",
    "        loss = loss[1:]\n",
    "        plt.plot(np.arange(len(loss)), loss, label='alpha = {}'.format(a))\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('Q(w)')\n",
    "        plt.legend()\n",
    "        \n",
    "fig = plt.figure()\n",
    "lin = fig.add_subplot(121)\n",
    "lin.title.set_text('linear regression')\n",
    "plot_a('lin_reg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot(reg_type):\n",
    "    ws = []\n",
    "    cs = np.logspace(-3, 3, 10)\n",
    "    for c in cs:\n",
    "        reg = MySGDClassifier(batch_generator=batch_generator, \n",
    "                                  max_epoch=1,C=c, alpha=0.0001, model_type=reg_type)\n",
    "        reg.fit(X, y)\n",
    "        ws.append(np.mean(reg.weights))\n",
    "    plt.plot(np.arange(len(ws)), ws)\n",
    "\n",
    "fig = plt.figure()\n",
    "lin = fig.add_subplot(121)\n",
    "lin.title.set_text('linear regression')\n",
    "plot('lin_reg')\n",
    "\n",
    "'''log = fig.add_subplot(122)\n",
    "lin.title.set_text('logistic regression')\n",
    "plot('log_reg')'''\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (3  балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук прислать Ваш ник на kaggle, под которым Вы залили решение, которое побило первый бейзлайн. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
